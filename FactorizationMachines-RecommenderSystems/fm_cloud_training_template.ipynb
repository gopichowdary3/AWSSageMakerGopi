{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Factorization Machines - Movie Recommendation Model</h2>\n",
    "Input Features: [userId, moveId] <br>\n",
    "Target: rating <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# SageMaker SDK Documentation: http://sagemaker.readthedocs.io/en/latest/estimators.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your bucket name\n",
    "bucket_name = 'sagemaker-gopi'\n",
    "training_file_key = 'movie/user_movie_train.recordio'\n",
    "test_file_key = 'movie/user_movie_test.recordio'\n",
    "\n",
    "s3_model_output_location = r's3://{0}/movie/model'.format(bucket_name)\n",
    "s3_training_file_location = r's3://{0}/{1}'.format(bucket_name,training_file_key)\n",
    "s3_test_file_location = r's3://{0}/{1}'.format(bucket_name,test_file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dimension: Number of unique users + Number of unique movies in our dataset\n",
    "dim_movie = 0\n",
    "\n",
    "# Update movie dimension - from file used for training \n",
    "with open(r'ml-latest-small/movie_dimension.txt','r') as f:\n",
    "    dim_movie = int(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-gopi/movie/model\n",
      "s3://sagemaker-gopi/movie/user_movie_train.recordio\n",
      "s3://sagemaker-gopi/movie/user_movie_test.recordio\n"
     ]
    }
   ],
   "source": [
    "print(s3_model_output_location)\n",
    "print(s3_training_file_location)\n",
    "print(s3_test_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and Reading from S3 is just as easy\n",
    "# files are referred as objects in S3.  \n",
    "# file name is referred as key name in S3\n",
    "# Files stored in S3 are automatically replicated across 3 different availability zones \n",
    "# in the region where the bucket was created.\n",
    "\n",
    "# http://boto3.readthedocs.io/en/latest/guide/s3.html\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename,'rb') as f: # Read in binary mode\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_s3(r'ml-latest-small/user_movie_train.recordio',bucket_name,training_file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_s3(r'ml-latest-small/user_movie_test.recordio',bucket_name,test_file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Algorithm Docker Image\n",
    "### AWS Maintains a separate image for every region and algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint uri: s3://sagemaker-gopi/movie/checkpoints/fm-movie-v4\n"
     ]
    }
   ],
   "source": [
    "# Use Spot Instance - Save up to 90% of training cost by using spot instances when compared to on-demand instances\n",
    "# Reference: https://github.com/aws-samples/amazon-sagemaker-managed-spot-training/blob/main/xgboost_built_in_managed_spot_training_checkpointing/xgboost_built_in_managed_spot_training_checkpointing.ipynb\n",
    "\n",
    "# if you are still on two-month free-tier you can use the on-demand instance by setting:\n",
    "#   use_spot_instances = False\n",
    "\n",
    "# We will use spot for training\n",
    "use_spot_instances = True\n",
    "max_run = 3600 # in seconds\n",
    "max_wait = 3600 if use_spot_instances else None # in seconds\n",
    "\n",
    "job_name = 'fm-movie-v4'\n",
    "\n",
    "checkpoint_s3_uri = None\n",
    "\n",
    "if use_spot_instances:\n",
    "    checkpoint_s3_uri = f's3://{bucket_name}/movie/checkpoints/{job_name}'\n",
    "    \n",
    "print (f'Checkpoint uri: {checkpoint_s3_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::512185592969:role/onshore-sagemaker-role\n"
     ]
    }
   ],
   "source": [
    "# This role contains the permissions needed to train, deploy models\n",
    "# SageMaker Service is trusted to assume this role\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FM Container 382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:1\n"
     ]
    }
   ],
   "source": [
    "# https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html#sagemaker.image_uris.retrieve\n",
    "\n",
    "# SDK 2 uses image_uris.retrieve the container image location\n",
    "\n",
    "# Use factorization-machines\n",
    "container = sagemaker.image_uris.retrieve(\"factorization-machines\",sess.boto_region_name)\n",
    "\n",
    "print (f'Using FM Container {container}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training job\n",
    "# Specify type and number of instances to use\n",
    "# S3 location where final artifacts needs to be stored\n",
    "\n",
    "#   Reference: http://sagemaker.readthedocs.io/en/latest/estimators.html\n",
    "\n",
    "# SDK 2.x version does not require train prefix for instance count and type\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(container,\n",
    "                                          role,                                        \n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.xlarge',\n",
    "                                          output_path=s3_model_output_location,\n",
    "                                          sagemaker_session=sess,\n",
    "                                          base_job_name = job_name,\n",
    "                                          use_spot_instances=use_spot_instances,\n",
    "                                          max_run=max_run,\n",
    "                                          max_wait=max_wait,\n",
    "                                          checkpoint_s3_uri=checkpoint_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Configuration after Model Tuning\n",
    "### Refer to Hyperparameter Tuning Lecture on how to optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(feature_dim=dim_movie,\n",
    "                              num_factors=8,\n",
    "                              predictor_type='regressor', \n",
    "                              mini_batch_size=994,\n",
    "                              epochs=91,\n",
    "                              bias_init_method='normal',\n",
    "                              bias_lr=0.21899531189430518,\n",
    "                              factors_init_method='normal',\n",
    "                              factors_lr=5.357593337770278e-05,\n",
    "                              linear_init_method='normal',\n",
    "                              linear_lr=0.00021524948053767607)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_dim': 10334,\n",
       " 'num_factors': 8,\n",
       " 'predictor_type': 'regressor',\n",
       " 'mini_batch_size': 994,\n",
       " 'epochs': 91,\n",
       " 'bias_init_method': 'normal',\n",
       " 'bias_lr': 0.21899531189430518,\n",
       " 'factors_init_method': 'normal',\n",
       " 'factors_lr': 5.357593337770278e-05,\n",
       " 'linear_init_method': 'normal',\n",
       " 'linear_lr': 0.00021524948053767607}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: fm-movie-v4-2023-06-09-14-05-39-444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-09 14:05:40 Starting - Starting the training job......\n",
      "2023-06-09 14:06:24 Starting - Preparing the instances for training......\n",
      "2023-06-09 14:07:42 Downloading - Downloading input data\n",
      "2023-06-09 14:07:42 Training - Downloading the training image...............\n",
      "2023-06-09 14:10:08 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'bias_init_method': 'normal', 'bias_lr': '0.21899531189430518', 'epochs': '91', 'factors_init_method': 'normal', 'factors_lr': '5.357593337770278e-05', 'feature_dim': '10334', 'linear_init_method': 'normal', 'linear_lr': '0.00021524948053767607', 'mini_batch_size': '994', 'num_factors': '8', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] Final configuration: {'epochs': '91', 'mini_batch_size': '994', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.21899531189430518', 'linear_lr': '0.00021524948053767607', 'factors_lr': '5.357593337770278e-05', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '10334', 'num_factors': '8', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 WARNING 140199547356992] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] Using default worker.\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:20.390] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:20.395] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 7, \"num_examples\": 1, \"num_bytes\": 63616}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319820.3870003, \"EndTime\": 1686319820.429167, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 36.037445068359375, \"count\": 1, \"min\": 36.037445068359375, \"max\": 36.037445068359375}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319820.4292903, \"EndTime\": 1686319820.4293272, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 994.0, \"count\": 1, \"min\": 994, \"max\": 994}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 994.0, \"count\": 1, \"min\": 994, \"max\": 994}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[14:10:20] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.238.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:306: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use kv.row_sparse_pull() or module.prepare() with row_ids.\u001b[0m\n",
      "\u001b[34m[14:10:20] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.238.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:306: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use kv.row_sparse_pull() or module.prepare() with row_ids.\u001b[0m\n",
      "\u001b[34m[14:10:20] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.238.0/AL2_x86_64/generic-flavor/src/src/operator/././../common/utils.h:450: Optimizer with lazy_update = True detected. Be aware that lazy update with row_sparse gradient is different from standard update, and may lead to different empirical results. See https://mxnet.incubator.apache.org/api/python/optimization/optimization.html for more details.\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=3.646887842133151\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=13.299790933098592\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=3.500805369325327\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:20.671] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 226, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.5197361996021286\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=0, train mse <loss>=2.309598116381121\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=1.1554540220107539\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319820.4292421, \"EndTime\": 1686319820.6716006, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"update.time\": {\"sum\": 242.07282066345215, \"count\": 1, \"min\": 242.07282066345215, \"max\": 242.07282066345215}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #progress_metric: host=algo-1, completed 1.098901098901099 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319820.4295053, \"EndTime\": 1686319820.6717699, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 71579.0, \"count\": 1, \"min\": 71579, \"max\": 71579}, \"Total Batches Seen\": {\"sum\": 73.0, \"count\": 1, \"min\": 73, \"max\": 73}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=291265.09337937617 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=1.0234240604966414\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=1.047396807603433\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.8189823756995096\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:20.891] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 217, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.1155756837454913\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.2445091061642206\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.8795603712266546\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319820.6716614, \"EndTime\": 1686319820.8925273, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 220.5798625946045, \"count\": 1, \"min\": 220.5798625946045, \"max\": 220.5798625946045}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #progress_metric: host=algo-1, completed 2.197802197802198 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319820.671927, \"EndTime\": 1686319820.8927217, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 142164.0, \"count\": 1, \"min\": 142164, \"max\": 142164}, \"Total Batches Seen\": {\"sum\": 145.0, \"count\": 1, \"min\": 145, \"max\": 145}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=319560.8880848579 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=1.0251199438229002\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=1.0508708992234659\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:20 INFO 140199547356992] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.82658352458501\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:21.114] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 219, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.1164356710480259\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.246428607588456\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.8795249362453476\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319820.8925862, \"EndTime\": 1686319821.115052, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 222.14245796203613, \"count\": 1, \"min\": 222.14245796203613, \"max\": 222.14245796203613}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #progress_metric: host=algo-1, completed 3.2967032967032965 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319820.892888, \"EndTime\": 1686319821.115371, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 212749.0, \"count\": 1, \"min\": 212749, \"max\": 212749}, \"Total Batches Seen\": {\"sum\": 217.0, \"count\": 1, \"min\": 217, \"max\": 217}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=317121.31692320877 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=1.0249226177880313\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=1.0504663724534709\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=0.8286705093844315\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:21.334] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 217, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=1.1143875756921624\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=3, train mse <loss>=1.2418596688570551\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=0.877086690573945\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319821.1151102, \"EndTime\": 1686319821.3350663, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 219.42377090454102, \"count\": 1, \"min\": 219.42377090454102, \"max\": 219.42377090454102}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #progress_metric: host=algo-1, completed 4.395604395604396 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319821.1156209, \"EndTime\": 1686319821.3352566, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 283334.0, \"count\": 1, \"min\": 283334, \"max\": 283334}, \"Total Batches Seen\": {\"sum\": 289.0, \"count\": 1, \"min\": 289, \"max\": 289}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=321224.14445459534 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=1.0248812188293173\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=1.050381512709067\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=0.8302560111646441\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:21.562] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 225, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=1.1115614940453293\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=4, train mse <loss>=1.2355689550442848\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=0.8743470121810507\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319821.3351278, \"EndTime\": 1686319821.5630646, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 227.54597663879395, \"count\": 1, \"min\": 227.54597663879395, \"max\": 227.54597663879395}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #progress_metric: host=algo-1, completed 5.4945054945054945 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319821.3354905, \"EndTime\": 1686319821.5633183, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 353919.0, \"count\": 1, \"min\": 353919, \"max\": 353919}, \"Total Batches Seen\": {\"sum\": 361.0, \"count\": 1, \"min\": 361, \"max\": 361}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=309654.67135106755 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=1.0236996550138573\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=1.0479609836754904\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=0.8298427036830357\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:21.800] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 235, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=1.1083176315578191\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=5, train mse <loss>=1.2283679724219339\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=0.8714153077440883\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319821.5631447, \"EndTime\": 1686319821.8011441, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 237.52546310424805, \"count\": 1, \"min\": 237.52546310424805, \"max\": 237.52546310424805}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #progress_metric: host=algo-1, completed 6.593406593406593 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319821.5635912, \"EndTime\": 1686319821.8013937, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 424504.0, \"count\": 1, \"min\": 424504, \"max\": 424504}, \"Total Batches Seen\": {\"sum\": 433.0, \"count\": 1, \"min\": 433, \"max\": 433}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=296667.86364629323 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=1.021472198468817\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=1.0434054522447183\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:21 INFO 140199547356992] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=0.827937350666499\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:22.089] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 285, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=1.1048948934682883\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=6, train mse <loss>=1.2207927256123001\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=0.8683295408778081\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319821.8012204, \"EndTime\": 1686319822.090116, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 288.44690322875977, \"count\": 1, \"min\": 288.44690322875977, \"max\": 288.44690322875977}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #progress_metric: host=algo-1, completed 7.6923076923076925 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319821.8016446, \"EndTime\": 1686319822.090365, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 495089.0, \"count\": 1, \"min\": 495089, \"max\": 495089}, \"Total Batches Seen\": {\"sum\": 505.0, \"count\": 1, \"min\": 505, \"max\": 505}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=244365.7309636458 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=1.018587458468458\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=1.0375204105492328\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=0.8251580405283262\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:22.379] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 287, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=1.1014064001742032\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=7, train mse <loss>=1.213096058344697\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=0.8651623255769971\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319822.0902026, \"EndTime\": 1686319822.3805337, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 289.8094654083252, \"count\": 1, \"min\": 289.8094654083252, \"max\": 289.8094654083252}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #progress_metric: host=algo-1, completed 8.791208791208792 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319822.0906777, \"EndTime\": 1686319822.3808355, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 565674.0, \"count\": 1, \"min\": 565674, \"max\": 565674}, \"Total Batches Seen\": {\"sum\": 577.0, \"count\": 1, \"min\": 577, \"max\": 577}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=243142.51653431822 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=1.0153375376730998\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=1.0309103154080734\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=0.8218921684403295\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:22.623] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 240, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=1.097900645645606\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=8, train mse <loss>=1.2053858277090386\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=0.8619507223121171\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319822.3806338, \"EndTime\": 1686319822.6238112, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 242.6435947418213, \"count\": 1, \"min\": 242.6435947418213, \"max\": 242.6435947418213}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #progress_metric: host=algo-1, completed 9.89010989010989 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319822.3811414, \"EndTime\": 1686319822.6240253, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 636259.0, \"count\": 1, \"min\": 636259, \"max\": 636259}, \"Total Batches Seen\": {\"sum\": 649.0, \"count\": 1, \"min\": 649, \"max\": 649}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=290475.4945408592 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=1.0118971849157024\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=1.0239359128403231\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=0.8183543399066273\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:22.883] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 257, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=1.0944008632151678\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=9, train mse <loss>=1.1977132494061047\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=0.8587238960769281\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319822.6238859, \"EndTime\": 1686319822.885449, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 261.16037368774414, \"count\": 1, \"min\": 261.16037368774414, \"max\": 261.16037368774414}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #progress_metric: host=algo-1, completed 10.989010989010989 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319822.6242442, \"EndTime\": 1686319822.8860946, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 706844.0, \"count\": 1, \"min\": 706844, \"max\": 706844}, \"Total Batches Seen\": {\"sum\": 721.0, \"count\": 1, \"min\": 721, \"max\": 721}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=269194.21107092133 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=1.0083672757125401\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=1.01680456272793\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:22 INFO 140199547356992] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=0.8146637392715669\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:23.179] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 290, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=1.0909204291235222\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=10, train mse <loss>=1.1901073826790496\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=0.8554959347283789\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319822.8857317, \"EndTime\": 1686319823.1806939, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 293.89071464538574, \"count\": 1, \"min\": 293.89071464538574, \"max\": 293.89071464538574}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #progress_metric: host=algo-1, completed 12.087912087912088 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319822.8867629, \"EndTime\": 1686319823.1810706, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 777429.0, \"count\": 1, \"min\": 777429, \"max\": 777429}, \"Total Batches Seen\": {\"sum\": 793.0, \"count\": 1, \"min\": 793, \"max\": 793}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=239730.3112190777 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=1.0048066333722658\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=1.009636370468907\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=0.8108884631028358\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:23.478] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 295, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=1.0874684160975043\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=11, train mse <loss>=1.1825875560096146\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=0.8522780597702971\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319823.1807523, \"EndTime\": 1686319823.480507, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 299.0117073059082, \"count\": 1, \"min\": 299.0117073059082, \"max\": 299.0117073059082}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #progress_metric: host=algo-1, completed 13.186813186813186 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319823.181447, \"EndTime\": 1686319823.4809499, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 848014.0, \"count\": 1, \"min\": 848014, \"max\": 848014}, \"Total Batches Seen\": {\"sum\": 865.0, \"count\": 1, \"min\": 865, \"max\": 865}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=235559.31200803298 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=1.0012504389043981\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=1.00250244140625\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=0.8070690991653043\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:23.765] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 281, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=1.0840516296352656\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=12, train mse <loss>=1.1751679357148752\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=0.8490731435255958\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319823.4805942, \"EndTime\": 1686319823.7663083, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 284.99317169189453, \"count\": 1, \"min\": 284.99317169189453, \"max\": 284.99317169189453}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #progress_metric: host=algo-1, completed 14.285714285714286 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319823.4812732, \"EndTime\": 1686319823.7665646, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 918599.0, \"count\": 1, \"min\": 918599, \"max\": 918599}, \"Total Batches Seen\": {\"sum\": 937.0, \"count\": 1, \"min\": 937, \"max\": 937}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=247284.92112573943 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=0.997720188145768\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=0.9954455738336268\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:23 INFO 140199547356992] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=0.8032308229258363\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:24.057] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 288, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=1.080675325879457\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=13, train mse <loss>=1.1678591599646708\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=0.8458918795978999\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319823.766385, \"EndTime\": 1686319824.058189, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 291.3076877593994, \"count\": 1, \"min\": 291.3076877593994, \"max\": 291.3076877593994}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #progress_metric: host=algo-1, completed 15.384615384615385 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319823.766856, \"EndTime\": 1686319824.0583632, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 989184.0, \"count\": 1, \"min\": 989184, \"max\": 989184}, \"Total Batches Seen\": {\"sum\": 1009.0, \"count\": 1, \"min\": 1009, \"max\": 1009}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=242051.5521863582 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=0.9942294359399421\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=0.9884921712894554\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=0.7993898449289487\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:24.332] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 271, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=1.077343775774561\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=14, train mse <loss>=1.1606696112001873\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=0.8427363557088516\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319824.0582452, \"EndTime\": 1686319824.3335066, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 274.9340534210205, \"count\": 1, \"min\": 274.9340534210205, \"max\": 274.9340534210205}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #progress_metric: host=algo-1, completed 16.483516483516482 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319824.0585446, \"EndTime\": 1686319824.3341525, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1059769.0, \"count\": 1, \"min\": 1059769, \"max\": 1059769}, \"Total Batches Seen\": {\"sum\": 1081.0, \"count\": 1, \"min\": 1081, \"max\": 1081}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=255770.35034898398 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=0.9907870283501268\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=0.981658935546875\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=0.795578340649365\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:24.636] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 299, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=1.0740602951512663\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=15, train mse <loss>=1.1536055176204252\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=0.8396096147468579\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319824.3337893, \"EndTime\": 1686319824.637534, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 302.3393154144287, \"count\": 1, \"min\": 302.3393154144287, \"max\": 302.3393154144287}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #progress_metric: host=algo-1, completed 17.582417582417584 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319824.3351684, \"EndTime\": 1686319824.6377647, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1130354.0, \"count\": 1, \"min\": 1130354, \"max\": 1130354}, \"Total Batches Seen\": {\"sum\": 1153.0, \"count\": 1, \"min\": 1153, \"max\": 1153}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=233169.21149877924 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=0.9873985392884947\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=0.974955875389053\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=0.7918195206394618\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:24.859] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 218, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=1.0708276131168524\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=16, train mse <loss>=1.1466717770135353\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=0.836511782930384\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319824.6376119, \"EndTime\": 1686319824.8600478, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 221.7562198638916, \"count\": 1, \"min\": 221.7562198638916, \"max\": 221.7562198638916}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #progress_metric: host=algo-1, completed 18.681318681318682 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319824.6382241, \"EndTime\": 1686319824.8602765, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1200939.0, \"count\": 1, \"min\": 1200939, \"max\": 1200939}, \"Total Batches Seen\": {\"sum\": 1225.0, \"count\": 1, \"min\": 1225, \"max\": 1225}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=317721.6586285128 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=0.9840678016232671\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=0.9683894381916499\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:24 INFO 140199547356992] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=0.7880852620606451\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:25.082] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 220, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=1.0676478214379697\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=17, train mse <loss>=1.1398718706212427\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=0.8334454042437677\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319824.8601098, \"EndTime\": 1686319825.0834255, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 222.67770767211914, \"count\": 1, \"min\": 222.67770767211914, \"max\": 222.67770767211914}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #progress_metric: host=algo-1, completed 19.78021978021978 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319824.8607225, \"EndTime\": 1686319825.083667, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1271524.0, \"count\": 1, \"min\": 1271524, \"max\": 1271524}, \"Total Batches Seen\": {\"sum\": 1297.0, \"count\": 1, \"min\": 1297, \"max\": 1297}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=316442.38112899795 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=0.9807970940759675\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=0.9619629397478622\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=0.7844097053021253\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:25.311] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 225, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=1.0645224832202624\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=18, train mse <loss>=1.133208117281434\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=0.8304129484753574\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319825.0835018, \"EndTime\": 1686319825.3125572, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 228.4383773803711, \"count\": 1, \"min\": 228.4383773803711, \"max\": 228.4383773803711}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #progress_metric: host=algo-1, completed 20.87912087912088 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319825.084093, \"EndTime\": 1686319825.3127952, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1342109.0, \"count\": 1, \"min\": 1342109, \"max\": 1342109}, \"Total Batches Seen\": {\"sum\": 1369.0, \"count\": 1, \"min\": 1369, \"max\": 1369}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=308472.50867678394 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=0.9775879299385011\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=0.9556781607614436\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=0.7807944468569228\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:25.552] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 236, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=1.061452862025449\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=19, train mse <loss>=1.126682178302017\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=0.827415849228127\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319825.3126378, \"EndTime\": 1686319825.5530336, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 239.67838287353516, \"count\": 1, \"min\": 239.67838287353516, \"max\": 239.67838287353516}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #progress_metric: host=algo-1, completed 21.978021978021978 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319825.3133273, \"EndTime\": 1686319825.5532594, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1412694.0, \"count\": 1, \"min\": 1412694, \"max\": 1412694}, \"Total Batches Seen\": {\"sum\": 1441.0, \"count\": 1, \"min\": 1441, \"max\": 1441}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=294056.53959689947 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=20, batch=0 train rmse <loss>=0.9744412638387956\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=20, batch=0 train mse <loss>=0.9495357766717493\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=20, batch=0 train absolute_loss <loss>=0.777328828930615\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:25.773] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 42, \"duration\": 217, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=20, train rmse <loss>=1.0584396851427709\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=20, train mse <loss>=1.1202945670851279\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=20, train absolute_loss <loss>=0.8244568304330104\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319825.5530963, \"EndTime\": 1686319825.773573, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 219.8812961578369, \"count\": 1, \"min\": 219.8812961578369, \"max\": 219.8812961578369}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #progress_metric: host=algo-1, completed 23.076923076923077 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319825.5536675, \"EndTime\": 1686319825.7737718, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1483279.0, \"count\": 1, \"min\": 1483279, \"max\": 1483279}, \"Total Batches Seen\": {\"sum\": 1513.0, \"count\": 1, \"min\": 1513, \"max\": 1513}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=320535.87531736365 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=21, batch=0 train rmse <loss>=0.9713573554770788\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=21, batch=0 train mse <loss>=0.9435351120394241\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=21, batch=0 train absolute_loss <loss>=0.7739536584742832\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:25.987] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 211, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=21, train rmse <loss>=1.0554835188009322\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=21, train mse <loss>=1.1140454584603978\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=21, train absolute_loss <loss>=0.8215354690875832\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319825.773632, \"EndTime\": 1686319825.987949, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 213.76514434814453, \"count\": 1, \"min\": 213.76514434814453, \"max\": 213.76514434814453}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #progress_metric: host=algo-1, completed 24.175824175824175 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319825.7741604, \"EndTime\": 1686319825.988152, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1553864.0, \"count\": 1, \"min\": 1553864, \"max\": 1553864}, \"Total Batches Seen\": {\"sum\": 1585.0, \"count\": 1, \"min\": 1585, \"max\": 1585}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 23.0, \"count\": 1, \"min\": 23, \"max\": 23}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=329686.24131671846 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=22, batch=0 train rmse <loss>=0.9683363605764167\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=22, batch=0 train mse <loss>=0.9376753072143801\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:25 INFO 140199547356992] #quality_metric: host=algo-1, epoch=22, batch=0 train absolute_loss <loss>=0.7706397687885124\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:26.209] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 46, \"duration\": 218, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=22, train rmse <loss>=1.0525846573934177\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=22, train mse <loss>=1.1079344609800186\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=22, train absolute_loss <loss>=0.8186551449471023\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319825.988024, \"EndTime\": 1686319826.2099466, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 221.32515907287598, \"count\": 1, \"min\": 221.32515907287598, \"max\": 221.32515907287598}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #progress_metric: host=algo-1, completed 25.274725274725274 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319825.988596, \"EndTime\": 1686319826.2101889, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1624449.0, \"count\": 1, \"min\": 1624449, \"max\": 1624449}, \"Total Batches Seen\": {\"sum\": 1657.0, \"count\": 1, \"min\": 1657, \"max\": 1657}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=318368.0940818183 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=23, batch=0 train rmse <loss>=0.96537797928917\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=23, batch=0 train mse <loss>=0.9319546428964411\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=23, batch=0 train absolute_loss <loss>=0.7674092037577025\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:26.431] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 48, \"duration\": 218, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=23, train rmse <loss>=1.0497430630281512\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=23, train mse <loss>=1.1019604983757252\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=23, train absolute_loss <loss>=0.8158205894983617\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319826.2100203, \"EndTime\": 1686319826.4317276, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 221.04859352111816, \"count\": 1, \"min\": 221.04859352111816, \"max\": 221.04859352111816}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #progress_metric: host=algo-1, completed 26.373626373626372 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319826.210653, \"EndTime\": 1686319826.431947, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1695034.0, \"count\": 1, \"min\": 1695034, \"max\": 1695034}, \"Total Batches Seen\": {\"sum\": 1729.0, \"count\": 1, \"min\": 1729, \"max\": 1729}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 25.0, \"count\": 1, \"min\": 25, \"max\": 25}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=318815.8489265654 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=24, batch=0 train rmse <loss>=0.9624820553149653\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=24, batch=0 train mse <loss>=0.9263717068033199\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=24, batch=0 train absolute_loss <loss>=0.7643423406650842\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:26.660] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 226, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=24, train rmse <loss>=1.0469586981824734\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=24, train mse <loss>=1.0961225156999397\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=24, train absolute_loss <loss>=0.8130323630490081\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319826.431803, \"EndTime\": 1686319826.6608725, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 228.4407615661621, \"count\": 1, \"min\": 228.4407615661621, \"max\": 228.4407615661621}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #progress_metric: host=algo-1, completed 27.47252747252747 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319826.4324079, \"EndTime\": 1686319826.661079, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1765619.0, \"count\": 1, \"min\": 1765619, \"max\": 1765619}, \"Total Batches Seen\": {\"sum\": 1801.0, \"count\": 1, \"min\": 1801, \"max\": 1801}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=308531.6596235153 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=25, batch=0 train rmse <loss>=0.9596478103891825\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=25, batch=0 train mse <loss>=0.9209239199847523\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=25, batch=0 train absolute_loss <loss>=0.7614038724534709\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:26.881] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 52, \"duration\": 217, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=25, train rmse <loss>=1.044231264383609\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=25, train mse <loss>=1.0904189335161905\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=25, train absolute_loss <loss>=0.8102913806829147\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319826.6609478, \"EndTime\": 1686319826.8817754, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 220.1399803161621, \"count\": 1, \"min\": 220.1399803161621, \"max\": 220.1399803161621}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #progress_metric: host=algo-1, completed 28.571428571428573 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319826.6616096, \"EndTime\": 1686319826.881998, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1836204.0, \"count\": 1, \"min\": 1836204, \"max\": 1836204}, \"Total Batches Seen\": {\"sum\": 1873.0, \"count\": 1, \"min\": 1873, \"max\": 1873}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 27.0, \"count\": 1, \"min\": 27, \"max\": 27}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=320120.65859088715 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=26, batch=0 train rmse <loss>=0.9568745412546031\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=26, batch=0 train mse <loss>=0.9156088877012073\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:26 INFO 140199547356992] #quality_metric: host=algo-1, epoch=26, batch=0 train absolute_loss <loss>=0.7586485711141852\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:27.111] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 54, \"duration\": 226, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=26, train rmse <loss>=1.0415602557903847\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=26, train mse <loss>=1.0848477664421314\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=26, train absolute_loss <loss>=0.8076024037358374\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319826.8818471, \"EndTime\": 1686319827.1115992, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 229.1262149810791, \"count\": 1, \"min\": 229.1262149810791, \"max\": 229.1262149810791}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #progress_metric: host=algo-1, completed 29.67032967032967 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319826.8824487, \"EndTime\": 1686319827.1119082, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1906789.0, \"count\": 1, \"min\": 1906789, \"max\": 1906789}, \"Total Batches Seen\": {\"sum\": 1945.0, \"count\": 1, \"min\": 1945, \"max\": 1945}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=307467.5275267374 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=27, batch=0 train rmse <loss>=0.9541618142879925\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=27, batch=0 train mse <loss>=0.9104247678453534\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=27, batch=0 train absolute_loss <loss>=0.7560096909582495\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:27.338] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 221, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=27, train rmse <loss>=1.0389452875529497\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=27, train mse <loss>=1.0794073105284812\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=27, train absolute_loss <loss>=0.8049677442230423\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319827.111702, \"EndTime\": 1686319827.3393803, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 226.790189743042, \"count\": 1, \"min\": 226.790189743042, \"max\": 226.790189743042}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #progress_metric: host=algo-1, completed 30.76923076923077 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319827.112565, \"EndTime\": 1686319827.339562, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1977374.0, \"count\": 1, \"min\": 1977374, \"max\": 1977374}, \"Total Batches Seen\": {\"sum\": 2017.0, \"count\": 1, \"min\": 2017, \"max\": 2017}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 29.0, \"count\": 1, \"min\": 29, \"max\": 29}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=310819.8218367092 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=28, batch=0 train rmse <loss>=0.9515083403821737\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=28, batch=0 train mse <loss>=0.9053681218168386\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=28, batch=0 train absolute_loss <loss>=0.753450205628301\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:27.566] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 58, \"duration\": 224, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=28, train rmse <loss>=1.036385629770208\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=28, train mse <loss>=1.074095173594191\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=28, train absolute_loss <loss>=0.8023887513175844\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319827.339453, \"EndTime\": 1686319827.56724, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 227.2024154663086, \"count\": 1, \"min\": 227.2024154663086, \"max\": 227.2024154663086}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #progress_metric: host=algo-1, completed 31.86813186813187 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319827.3400156, \"EndTime\": 1686319827.5674531, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2047959.0, \"count\": 1, \"min\": 2047959, \"max\": 2047959}, \"Total Batches Seen\": {\"sum\": 2089.0, \"count\": 1, \"min\": 2089, \"max\": 2089}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=310184.79584513337 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=29, batch=0 train rmse <loss>=0.9489134841967641\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=29, batch=0 train mse <loss>=0.9004368004904426\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=29, batch=0 train absolute_loss <loss>=0.7509918519908514\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:27.787] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 60, \"duration\": 217, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=29, train rmse <loss>=1.033880644828907\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=29, train mse <loss>=1.0689091877518366\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=29, train absolute_loss <loss>=0.7998662966091361\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319827.5673041, \"EndTime\": 1686319827.78812, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 220.15142440795898, \"count\": 1, \"min\": 220.15142440795898, \"max\": 220.15142440795898}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #progress_metric: host=algo-1, completed 32.967032967032964 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319827.5679476, \"EndTime\": 1686319827.788312, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2118544.0, \"count\": 1, \"min\": 2118544, \"max\": 2118544}, \"Total Batches Seen\": {\"sum\": 2161.0, \"count\": 1, \"min\": 2161, \"max\": 2161}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 31.0, \"count\": 1, \"min\": 31, \"max\": 31}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=320167.04842384846 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=30, batch=0 train rmse <loss>=0.946376203154968\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=30, batch=0 train mse <loss>=0.8956279178980131\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:27 INFO 140199547356992] #quality_metric: host=algo-1, epoch=30, batch=0 train absolute_loss <loss>=0.7486017177282445\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:28.020] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 229, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=30, train rmse <loss>=1.0314295185122702\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=30, train mse <loss>=1.0638468516584534\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=30, train absolute_loss <loss>=0.797399377172595\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319827.788181, \"EndTime\": 1686319828.0208642, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 232.086181640625, \"count\": 1, \"min\": 232.086181640625, \"max\": 232.086181640625}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #progress_metric: host=algo-1, completed 34.065934065934066 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319827.7887402, \"EndTime\": 1686319828.0210516, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2189129.0, \"count\": 1, \"min\": 2189129, \"max\": 2189129}, \"Total Batches Seen\": {\"sum\": 2233.0, \"count\": 1, \"min\": 2233, \"max\": 2233}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=303709.33418958 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=31, batch=0 train rmse <loss>=0.9438954002789819\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=31, batch=0 train mse <loss>=0.8909385266678194\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=31, batch=0 train absolute_loss <loss>=0.7462684431786029\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:28.230] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 64, \"duration\": 207, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=31, train rmse <loss>=1.029031407042855\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=31, train mse <loss>=1.058905636680598\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=31, train absolute_loss <loss>=0.7949891400609058\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319828.0209212, \"EndTime\": 1686319828.2314186, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 209.87963676452637, \"count\": 1, \"min\": 209.87963676452637, \"max\": 209.87963676452637}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #progress_metric: host=algo-1, completed 35.16483516483517 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319828.0215147, \"EndTime\": 1686319828.2316396, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2259714.0, \"count\": 1, \"min\": 2259714, \"max\": 2259714}, \"Total Batches Seen\": {\"sum\": 2305.0, \"count\": 1, \"min\": 2305, \"max\": 2305}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 33.0, \"count\": 1, \"min\": 33, \"max\": 33}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=335737.43383174285 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=32, batch=0 train rmse <loss>=0.9414703154467942\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=32, batch=0 train mse <loss>=0.8863663548674862\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=32, batch=0 train absolute_loss <loss>=0.7439898792167065\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:28.448] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 66, \"duration\": 214, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=32, train rmse <loss>=1.0266855538765831\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=32, train mse <loss>=1.0540832265388664\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=32, train absolute_loss <loss>=0.7926374180002977\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319828.2314885, \"EndTime\": 1686319828.4488568, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 216.68672561645508, \"count\": 1, \"min\": 216.68672561645508, \"max\": 216.68672561645508}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #progress_metric: host=algo-1, completed 36.26373626373626 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319828.2321467, \"EndTime\": 1686319828.4490159, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2330299.0, \"count\": 1, \"min\": 2330299, \"max\": 2330299}, \"Total Batches Seen\": {\"sum\": 2377.0, \"count\": 1, \"min\": 2377, \"max\": 2377}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=325335.46502798353 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=33, batch=0 train rmse <loss>=0.9390995495136079\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=33, batch=0 train mse <loss>=0.8819079638966613\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=33, batch=0 train absolute_loss <loss>=0.7417611749599158\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:28.687] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 236, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=33, train rmse <loss>=1.0243909493628607\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=33, train mse <loss>=1.0493768171365432\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=33, train absolute_loss <loss>=0.7903423714419219\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319828.4489143, \"EndTime\": 1686319828.688204, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 238.7106418609619, \"count\": 1, \"min\": 238.7106418609619, \"max\": 238.7106418609619}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #progress_metric: host=algo-1, completed 37.362637362637365 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319828.4494708, \"EndTime\": 1686319828.6884122, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2400884.0, \"count\": 1, \"min\": 2400884, \"max\": 2400884}, \"Total Batches Seen\": {\"sum\": 2449.0, \"count\": 1, \"min\": 2449, \"max\": 2449}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 35.0, \"count\": 1, \"min\": 35, \"max\": 35}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=295273.948246887 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=34, batch=0 train rmse <loss>=0.9367822039365585\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=34, batch=0 train mse <loss>=0.8775608976122359\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=34, batch=0 train absolute_loss <loss>=0.7395937514736859\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:28.907] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 70, \"duration\": 216, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=34, train rmse <loss>=1.0221466545807802\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=34, train mse <loss>=1.0447837834706808\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=34, train absolute_loss <loss>=0.7881022085952674\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319828.6882706, \"EndTime\": 1686319828.908294, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 219.37894821166992, \"count\": 1, \"min\": 219.37894821166992, \"max\": 219.37894821166992}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #progress_metric: host=algo-1, completed 38.46153846153846 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319828.6888916, \"EndTime\": 1686319828.9084527, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2471469.0, \"count\": 1, \"min\": 2471469, \"max\": 2471469}, \"Total Batches Seen\": {\"sum\": 2521.0, \"count\": 1, \"min\": 2521, \"max\": 2521}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=321350.36301509634 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=35, batch=0 train rmse <loss>=0.9345170991707321\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=35, batch=0 train mse <loss>=0.8733222086424799\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:28 INFO 140199547356992] #quality_metric: host=algo-1, epoch=35, batch=0 train absolute_loss <loss>=0.7374591174979565\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:29.135] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 72, \"duration\": 225, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=35, train rmse <loss>=1.0199518356302273\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=35, train mse <loss>=1.0403017470054703\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=35, train absolute_loss <loss>=0.7859172390367051\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319828.9083507, \"EndTime\": 1686319829.136246, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 227.32186317443848, \"count\": 1, \"min\": 227.32186317443848, \"max\": 227.32186317443848}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #progress_metric: host=algo-1, completed 39.56043956043956 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319828.9089015, \"EndTime\": 1686319829.1364079, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2542054.0, \"count\": 1, \"min\": 2542054, \"max\": 2542054}, \"Total Batches Seen\": {\"sum\": 2593.0, \"count\": 1, \"min\": 2593, \"max\": 2593}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 37.0, \"count\": 1, \"min\": 37, \"max\": 37}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=310131.181925222 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=36, batch=0 train rmse <loss>=0.9323031011547791\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=36, batch=0 train mse <loss>=0.8691890724228182\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=36, batch=0 train absolute_loss <loss>=0.7353585625078597\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:29.349] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 210, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=36, train rmse <loss>=1.017805385207667\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=36, train mse <loss>=1.0359278021577272\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=36, train absolute_loss <loss>=0.783783376017007\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319829.1363049, \"EndTime\": 1686319829.3495774, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 212.6002311706543, \"count\": 1, \"min\": 212.6002311706543, \"max\": 212.6002311706543}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #progress_metric: host=algo-1, completed 40.65934065934066 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319829.136955, \"EndTime\": 1686319829.3497372, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2612639.0, \"count\": 1, \"min\": 2612639, \"max\": 2612639}, \"Total Batches Seen\": {\"sum\": 2665.0, \"count\": 1, \"min\": 2665, \"max\": 2665}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=331585.30838990334 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=37, batch=0 train rmse <loss>=0.9301392215182437\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=37, batch=0 train mse <loss>=0.8651589714065644\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=37, batch=0 train absolute_loss <loss>=0.7332794373663858\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:29.579] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 76, \"duration\": 226, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=37, train rmse <loss>=1.0157063421031185\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=37, train mse <loss>=1.0316593733884971\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=37, train absolute_loss <loss>=0.7817019277735096\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319829.3496356, \"EndTime\": 1686319829.5796688, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 229.37560081481934, \"count\": 1, \"min\": 229.37560081481934, \"max\": 229.37560081481934}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #progress_metric: host=algo-1, completed 41.75824175824176 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319829.3502681, \"EndTime\": 1686319829.5799212, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2683224.0, \"count\": 1, \"min\": 2683224, \"max\": 2683224}, \"Total Batches Seen\": {\"sum\": 2737.0, \"count\": 1, \"min\": 2737, \"max\": 2737}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 39.0, \"count\": 1, \"min\": 39, \"max\": 39}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=307171.48384017596 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=38, batch=0 train rmse <loss>=0.9280242228638153\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=38, batch=0 train mse <loss>=0.8612289582219882\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=38, batch=0 train absolute_loss <loss>=0.7312380754252075\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:29.796] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 78, \"duration\": 213, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=38, train rmse <loss>=1.0136537455465822\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=38, train mse <loss>=1.0274939158606151\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=38, train absolute_loss <loss>=0.779673009437921\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319829.5797462, \"EndTime\": 1686319829.7967227, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 216.27497673034668, \"count\": 1, \"min\": 216.27497673034668, \"max\": 216.27497673034668}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #progress_metric: host=algo-1, completed 42.857142857142854 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319829.5804229, \"EndTime\": 1686319829.796948, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2753809.0, \"count\": 1, \"min\": 2753809, \"max\": 2753809}, \"Total Batches Seen\": {\"sum\": 2809.0, \"count\": 1, \"min\": 2809, \"max\": 2809}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=325812.0103712247 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=39, batch=0 train rmse <loss>=0.9259569149288289\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=39, batch=0 train mse <loss>=0.8573962083045146\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:29 INFO 140199547356992] #quality_metric: host=algo-1, epoch=39, batch=0 train absolute_loss <loss>=0.7292312222947057\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:30.030] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 230, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=39, train rmse <loss>=1.011646545877785\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=39, train mse <loss>=1.0234287337864536\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=39, train absolute_loss <loss>=0.7776933273905586\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319829.7967944, \"EndTime\": 1686319830.031253, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 233.68310928344727, \"count\": 1, \"min\": 233.68310928344727, \"max\": 233.68310928344727}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #progress_metric: host=algo-1, completed 43.956043956043956 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319829.7975423, \"EndTime\": 1686319830.0314832, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2824394.0, \"count\": 1, \"min\": 2824394, \"max\": 2824394}, \"Total Batches Seen\": {\"sum\": 2881.0, \"count\": 1, \"min\": 2881, \"max\": 2881}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 41.0, \"count\": 1, \"min\": 41, \"max\": 41}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=301571.2898743822 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=40, batch=0 train rmse <loss>=0.9239362886644484\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=40, batch=0 train mse <loss>=0.853658265511035\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=40, batch=0 train absolute_loss <loss>=0.7272519393705986\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2023-06-09 14:10:30.252] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 82, \"duration\": 218, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=40, train rmse <loss>=1.0096837523553426\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=40, train mse <loss>=1.0194612797703648\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=40, train absolute_loss <loss>=0.775761929875968\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319830.0313244, \"EndTime\": 1686319830.2530115, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 221.03333473205566, \"count\": 1, \"min\": 221.03333473205566, \"max\": 221.03333473205566}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #progress_metric: host=algo-1, completed 45.05494505494506 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319830.031952, \"EndTime\": 1686319830.2532723, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2894979.0, \"count\": 1, \"min\": 2894979, \"max\": 2894979}, \"Total Batches Seen\": {\"sum\": 2953.0, \"count\": 1, \"min\": 2953, \"max\": 2953}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=318761.612548263 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=41, batch=0 train rmse <loss>=0.9219613522821976\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=41, batch=0 train mse <loss>=0.8500127351020184\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=41, batch=0 train absolute_loss <loss>=0.7253031740246164\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:30.468] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 84, \"duration\": 212, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=41, train rmse <loss>=1.0077643766397704\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=41, train mse <loss>=1.0155890388241449\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=41, train absolute_loss <loss>=0.7738766106294888\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319830.2530975, \"EndTime\": 1686319830.4690986, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 215.30532836914062, \"count\": 1, \"min\": 215.30532836914062, \"max\": 215.30532836914062}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #progress_metric: host=algo-1, completed 46.15384615384615 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319830.2537668, \"EndTime\": 1686319830.4693344, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2965564.0, \"count\": 1, \"min\": 2965564, \"max\": 2965564}, \"Total Batches Seen\": {\"sum\": 3025.0, \"count\": 1, \"min\": 3025, \"max\": 3025}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 43.0, \"count\": 1, \"min\": 43, \"max\": 43}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=327267.7856263251 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=42, batch=0 train rmse <loss>=0.9200306981264273\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=42, batch=0 train mse <loss>=0.8464564854950013\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=42, batch=0 train absolute_loss <loss>=0.7233960403043259\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:30.689] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 217, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=42, train rmse <loss>=1.0058873700846138\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=42, train mse <loss>=1.011809401295741\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=42, train absolute_loss <loss>=0.7720358635578117\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319830.469169, \"EndTime\": 1686319830.689656, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 220.076322555542, \"count\": 1, \"min\": 220.076322555542, \"max\": 220.076322555542}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #progress_metric: host=algo-1, completed 47.252747252747255 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319830.4695559, \"EndTime\": 1686319830.6898196, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3036149.0, \"count\": 1, \"min\": 3036149, \"max\": 3036149}, \"Total Batches Seen\": {\"sum\": 3097.0, \"count\": 1, \"min\": 3097, \"max\": 3097}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=320323.28019406256 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=43, batch=0 train rmse <loss>=0.9181434017413543\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=43, batch=0 train mse <loss>=0.8429873061611859\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=43, batch=0 train absolute_loss <loss>=0.7215306610168826\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:30.912] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 88, \"duration\": 220, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=43, train rmse <loss>=1.0040517280821784\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=43, train mse <loss>=1.0081198726648084\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=43, train absolute_loss <loss>=0.7702385484306119\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319830.6897137, \"EndTime\": 1686319830.9128246, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 222.81336784362793, \"count\": 1, \"min\": 222.81336784362793, \"max\": 222.81336784362793}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #progress_metric: host=algo-1, completed 48.35164835164835 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319830.6899884, \"EndTime\": 1686319830.9129884, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3106734.0, \"count\": 1, \"min\": 3106734, \"max\": 3106734}, \"Total Batches Seen\": {\"sum\": 3169.0, \"count\": 1, \"min\": 3169, \"max\": 3169}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 45.0, \"count\": 1, \"min\": 45, \"max\": 45}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=316401.1219895372 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=44, batch=0 train rmse <loss>=0.9162983907862458\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=44, batch=0 train mse <loss>=0.8396027409574636\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:30 INFO 140199547356992] #quality_metric: host=algo-1, epoch=44, batch=0 train absolute_loss <loss>=0.7196975800113179\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:31.125] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 90, \"duration\": 210, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=44, train rmse <loss>=1.0022564738776256\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=44, train mse <loss>=1.0045180394296116\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=44, train absolute_loss <loss>=0.7684810339085293\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319830.9128845, \"EndTime\": 1686319831.1255915, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 212.41188049316406, \"count\": 1, \"min\": 212.41188049316406, \"max\": 212.41188049316406}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #progress_metric: host=algo-1, completed 49.45054945054945 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319830.9131563, \"EndTime\": 1686319831.125812, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3177319.0, \"count\": 1, \"min\": 3177319, \"max\": 3177319}, \"Total Batches Seen\": {\"sum\": 3241.0, \"count\": 1, \"min\": 3241, \"max\": 3241}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=331761.4359018034 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=45, batch=0 train rmse <loss>=0.9144946454451666\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=45, batch=0 train mse <loss>=0.836300456547881\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=45, batch=0 train absolute_loss <loss>=0.7178969200947874\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:31.336] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 208, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=45, train rmse <loss>=1.000500601686634\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=45, train mse <loss>=1.0010014539753167\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=45, train absolute_loss <loss>=0.7667666076414542\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319831.1256528, \"EndTime\": 1686319831.3369384, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 210.88457107543945, \"count\": 1, \"min\": 210.88457107543945, \"max\": 210.88457107543945}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #progress_metric: host=algo-1, completed 50.54945054945055 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319831.126026, \"EndTime\": 1686319831.3371682, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3247904.0, \"count\": 1, \"min\": 3247904, \"max\": 3247904}, \"Total Batches Seen\": {\"sum\": 3313.0, \"count\": 1, \"min\": 3313, \"max\": 3313}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 47.0, \"count\": 1, \"min\": 47, \"max\": 47}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=334108.9555503141 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=46, batch=0 train rmse <loss>=0.9127308294148312\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=46, batch=0 train mse <loss>=0.8330775669642857\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=46, batch=0 train absolute_loss <loss>=0.7161548391913984\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:31.568] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 94, \"duration\": 229, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=46, train rmse <loss>=0.9987830930594666\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=46, train mse <loss>=0.9975676669814351\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=46, train absolute_loss <loss>=0.7650959808874951\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319831.3370082, \"EndTime\": 1686319831.5692358, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 231.53114318847656, \"count\": 1, \"min\": 231.53114318847656, \"max\": 231.53114318847656}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #progress_metric: host=algo-1, completed 51.64835164835165 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319831.3376813, \"EndTime\": 1686319831.5694427, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3318489.0, \"count\": 1, \"min\": 3318489, \"max\": 3318489}, \"Total Batches Seen\": {\"sum\": 3385.0, \"count\": 1, \"min\": 3385, \"max\": 3385}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=304438.26432470954 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=47, batch=0 train rmse <loss>=0.9110062309921384\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=47, batch=0 train mse <loss>=0.8299323529065015\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=47, batch=0 train absolute_loss <loss>=0.714450705699038\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:31.785] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 96, \"duration\": 213, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=47, train rmse <loss>=0.9971030262178664\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=47, train mse <loss>=0.9942144448928271\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=47, train absolute_loss <loss>=0.7634683656340714\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319831.5692945, \"EndTime\": 1686319831.7864773, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 216.54677391052246, \"count\": 1, \"min\": 216.54677391052246, \"max\": 216.54677391052246}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #progress_metric: host=algo-1, completed 52.747252747252745 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319831.5699039, \"EndTime\": 1686319831.786732, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3389074.0, \"count\": 1, \"min\": 3389074, \"max\": 3389074}, \"Total Batches Seen\": {\"sum\": 3457.0, \"count\": 1, \"min\": 3457, \"max\": 3457}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 49.0, \"count\": 1, \"min\": 49, \"max\": 49}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=325373.36570343986 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=48, batch=0 train rmse <loss>=0.9093196552501569\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=48, batch=0 train mse <loss>=0.8268622354242643\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:31 INFO 140199547356992] #quality_metric: host=algo-1, epoch=48, batch=0 train absolute_loss <loss>=0.7127667125801685\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:32.009] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 220, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=48, train rmse <loss>=0.9954594761477839\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=48, train mse <loss>=0.9909395686524202\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=48, train absolute_loss <loss>=0.7618799079538537\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319831.7865493, \"EndTime\": 1686319832.0101206, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 223.13261032104492, \"count\": 1, \"min\": 223.13261032104492, \"max\": 223.13261032104492}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #progress_metric: host=algo-1, completed 53.84615384615385 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319831.7869616, \"EndTime\": 1686319832.0103638, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3459659.0, \"count\": 1, \"min\": 3459659, \"max\": 3459659}, \"Total Batches Seen\": {\"sum\": 3529.0, \"count\": 1, \"min\": 3529, \"max\": 3529}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=315783.5234687075 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=49, batch=0 train rmse <loss>=0.9076700287999053\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=49, batch=0 train mse <loss>=0.8238648811816209\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=49, batch=0 train absolute_loss <loss>=0.7111002808845259\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:32.235] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 100, \"duration\": 222, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=49, train rmse <loss>=0.9938513591703932\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=49, train mse <loss>=0.987740524124838\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=49, train absolute_loss <loss>=0.7603255556969203\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319832.010194, \"EndTime\": 1686319832.235808, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 225.14677047729492, \"count\": 1, \"min\": 225.14677047729492, \"max\": 225.14677047729492}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #progress_metric: host=algo-1, completed 54.94505494505494 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319832.0106359, \"EndTime\": 1686319832.2360082, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3530244.0, \"count\": 1, \"min\": 3530244, \"max\": 3530244}, \"Total Batches Seen\": {\"sum\": 3601.0, \"count\": 1, \"min\": 3601, \"max\": 3601}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 51.0, \"count\": 1, \"min\": 51, \"max\": 51}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=312989.8919222364 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=50, batch=0 train rmse <loss>=0.9060566046767693\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=50, batch=0 train mse <loss>=0.8209385708783954\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=50, batch=0 train absolute_loss <loss>=0.709466822910117\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:32.454] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 102, \"duration\": 216, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=50, train rmse <loss>=0.9922778712064845\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=50, train mse <loss>=0.9846153736860725\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=50, train absolute_loss <loss>=0.7588042053045269\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319832.2358832, \"EndTime\": 1686319832.4551065, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 218.81580352783203, \"count\": 1, \"min\": 218.81580352783203, \"max\": 218.81580352783203}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #progress_metric: host=algo-1, completed 56.043956043956044 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319832.236268, \"EndTime\": 1686319832.4553154, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 50, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3600829.0, \"count\": 1, \"min\": 3600829, \"max\": 3600829}, \"Total Batches Seen\": {\"sum\": 3673.0, \"count\": 1, \"min\": 3673, \"max\": 3673}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=322075.4211116067 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=51, batch=0 train rmse <loss>=0.904478117015965\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=51, batch=0 train mse <loss>=0.8180806641607458\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=51, batch=0 train absolute_loss <loss>=0.7078842070980571\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:32.676] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 219, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=51, train rmse <loss>=0.9907380805902913\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=51, train mse <loss>=0.9815619443317347\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=51, train absolute_loss <loss>=0.7573165535367151\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319832.4551725, \"EndTime\": 1686319832.67698, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 221.27008438110352, \"count\": 1, \"min\": 221.27008438110352, \"max\": 221.27008438110352}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #progress_metric: host=algo-1, completed 57.142857142857146 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319832.4556847, \"EndTime\": 1686319832.6772258, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 51, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3671414.0, \"count\": 1, \"min\": 3671414, \"max\": 3671414}, \"Total Batches Seen\": {\"sum\": 3745.0, \"count\": 1, \"min\": 3745, \"max\": 3745}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 53.0, \"count\": 1, \"min\": 53, \"max\": 53}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=318449.5973220778 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=52, batch=0 train rmse <loss>=0.9029337289753554\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=52, batch=0 train mse <loss>=0.8152893189213405\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=52, batch=0 train absolute_loss <loss>=0.7063322930748553\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:32.890] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 106, \"duration\": 211, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=52, train rmse <loss>=0.9892310334156583\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=52, train mse <loss>=0.9785780374726113\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=52, train absolute_loss <loss>=0.7558632988591816\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319832.6770556, \"EndTime\": 1686319832.8909535, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 213.31167221069336, \"count\": 1, \"min\": 213.31167221069336, \"max\": 213.31167221069336}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #progress_metric: host=algo-1, completed 58.24175824175824 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319832.677617, \"EndTime\": 1686319832.8911905, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 52, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3741999.0, \"count\": 1, \"min\": 3741999, \"max\": 3741999}, \"Total Batches Seen\": {\"sum\": 3817.0, \"count\": 1, \"min\": 3817, \"max\": 3817}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=330279.86898355157 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=53, batch=0 train rmse <loss>=0.9014225940439081\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=53, batch=0 train mse <loss>=0.8125626930528483\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:32 INFO 140199547356992] #quality_metric: host=algo-1, epoch=53, batch=0 train absolute_loss <loss>=0.7048219492737676\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:33.110] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 108, \"duration\": 217, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=53, train rmse <loss>=0.98775587801287\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=53, train mse <loss>=0.9756616745489757\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=53, train absolute_loss <loss>=0.7544428302808387\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319832.891027, \"EndTime\": 1686319833.1114862, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 219.8481559753418, \"count\": 1, \"min\": 219.8481559753418, \"max\": 219.8481559753418}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #progress_metric: host=algo-1, completed 59.34065934065934 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319832.8916128, \"EndTime\": 1686319833.111713, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 53, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3812584.0, \"count\": 1, \"min\": 3812584, \"max\": 3812584}, \"Total Batches Seen\": {\"sum\": 3889.0, \"count\": 1, \"min\": 3889, \"max\": 3889}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=320529.97572652606 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=54, batch=0 train rmse <loss>=0.8999436174688354\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=54, batch=0 train mse <loss>=0.8098985146228936\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=54, batch=0 train absolute_loss <loss>=0.7033582107881665\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:33.339] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 225, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=54, train rmse <loss>=0.9863116740539942\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=54, train mse <loss>=0.9728107183751925\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=54, train absolute_loss <loss>=0.7530549345948193\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319833.1115575, \"EndTime\": 1686319833.339653, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 227.68497467041016, \"count\": 1, \"min\": 227.68497467041016, \"max\": 227.68497467041016}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #progress_metric: host=algo-1, completed 60.43956043956044 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319833.1119452, \"EndTime\": 1686319833.3398151, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 54, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3883169.0, \"count\": 1, \"min\": 3883169, \"max\": 3883169}, \"Total Batches Seen\": {\"sum\": 3961.0, \"count\": 1, \"min\": 3961, \"max\": 3961}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=309636.53518271964 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=55, batch=0 train rmse <loss>=0.8984961379692242\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=55, batch=0 train mse <loss>=0.8072953099456112\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=55, batch=0 train absolute_loss <loss>=0.701944761832715\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:33.560] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 112, \"duration\": 218, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=55, train rmse <loss>=0.9848976783700294\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=55, train mse <loss>=0.9700234368586738\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=55, train absolute_loss <loss>=0.7516997832194448\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319833.3397114, \"EndTime\": 1686319833.5604792, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 220.4725742340088, \"count\": 1, \"min\": 220.4725742340088, \"max\": 220.4725742340088}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #progress_metric: host=algo-1, completed 61.53846153846154 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319833.3399854, \"EndTime\": 1686319833.5606632, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 55, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3953754.0, \"count\": 1, \"min\": 3953754, \"max\": 3953754}, \"Total Batches Seen\": {\"sum\": 4033.0, \"count\": 1, \"min\": 4033, \"max\": 4033}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 57.0, \"count\": 1, \"min\": 57, \"max\": 57}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=319730.3400608239 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=56, batch=0 train rmse <loss>=0.8970790417174094\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=56, batch=0 train mse <loss>=0.8047508070886255\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=56, batch=0 train absolute_loss <loss>=0.7005633041412538\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:33.780] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 114, \"duration\": 218, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=56, train rmse <loss>=0.9835130231931793\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=56, train mse <loss>=0.9672978667905873\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=56, train absolute_loss <loss>=0.7503731930359828\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319833.5605507, \"EndTime\": 1686319833.7814295, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 220.57342529296875, \"count\": 1, \"min\": 220.57342529296875, \"max\": 220.57342529296875}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #progress_metric: host=algo-1, completed 62.637362637362635 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319833.5608327, \"EndTime\": 1686319833.7816634, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 56, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4024339.0, \"count\": 1, \"min\": 4024339, \"max\": 4024339}, \"Total Batches Seen\": {\"sum\": 4105.0, \"count\": 1, \"min\": 4105, \"max\": 4105}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=319483.6417396972 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=57, batch=0 train rmse <loss>=0.8956915817171198\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=57, batch=0 train mse <loss>=0.802263409558916\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:33 INFO 140199547356992] #quality_metric: host=algo-1, epoch=57, batch=0 train absolute_loss <loss>=0.6992379079162475\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:34.014] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 230, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=57, train rmse <loss>=0.9821568231942746\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=57, train mse <loss>=0.9646320253470694\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=57, train absolute_loss <loss>=0.7490748774944044\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319833.7815013, \"EndTime\": 1686319834.0146263, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 232.54060745239258, \"count\": 1, \"min\": 232.54060745239258, \"max\": 232.54060745239258}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #progress_metric: host=algo-1, completed 63.73626373626374 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319833.782037, \"EndTime\": 1686319834.0148478, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 57, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4094924.0, \"count\": 1, \"min\": 4094924, \"max\": 4094924}, \"Total Batches Seen\": {\"sum\": 4177.0, \"count\": 1, \"min\": 4177, \"max\": 4177}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 59.0, \"count\": 1, \"min\": 59, \"max\": 59}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=303036.2980750531 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=58, batch=0 train rmse <loss>=0.8943329690109184\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=58, batch=0 train mse <loss>=0.7998314594598843\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=58, batch=0 train absolute_loss <loss>=0.6979492310307155\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:34.233] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 118, \"duration\": 215, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=58, train rmse <loss>=0.980828357344498\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=58, train mse <loss>=0.9620242665711063\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=58, train absolute_loss <loss>=0.7478041748005939\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319834.0147, \"EndTime\": 1686319834.233843, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 218.5072898864746, \"count\": 1, \"min\": 218.5072898864746, \"max\": 218.5072898864746}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #progress_metric: host=algo-1, completed 64.83516483516483 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319834.0153089, \"EndTime\": 1686319834.2340748, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 58, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4165509.0, \"count\": 1, \"min\": 4165509, \"max\": 4165509}, \"Total Batches Seen\": {\"sum\": 4249.0, \"count\": 1, \"min\": 4249, \"max\": 4249}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=322474.65105133364 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=59, batch=0 train rmse <loss>=0.8930022694711482\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=59, batch=0 train mse <loss>=0.7974530532806212\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=59, batch=0 train absolute_loss <loss>=0.6966827822403169\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:34.457] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 120, \"duration\": 221, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=59, train rmse <loss>=0.9795268099351381\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=59, train mse <loss>=0.9594727713817082\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=59, train absolute_loss <loss>=0.7465603438474816\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319834.2339144, \"EndTime\": 1686319834.4580176, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 223.50454330444336, \"count\": 1, \"min\": 223.50454330444336, \"max\": 223.50454330444336}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #progress_metric: host=algo-1, completed 65.93406593406593 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319834.2344859, \"EndTime\": 1686319834.4582555, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 59, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4236094.0, \"count\": 1, \"min\": 4236094, \"max\": 4236094}, \"Total Batches Seen\": {\"sum\": 4321.0, \"count\": 1, \"min\": 4321, \"max\": 4321}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 61.0, \"count\": 1, \"min\": 61, \"max\": 61}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=315287.48438764643 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=60, batch=0 train rmse <loss>=0.8916988161587068\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=60, batch=0 train mse <loss>=0.7951267787388393\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=60, batch=0 train absolute_loss <loss>=0.695444947517134\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:34.681] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 221, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=60, train rmse <loss>=0.9782514475709652\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=60, train mse <loss>=0.9569758946746888\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=60, train absolute_loss <loss>=0.7453422537858444\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319834.458089, \"EndTime\": 1686319834.6820652, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 223.54531288146973, \"count\": 1, \"min\": 223.54531288146973, \"max\": 223.54531288146973}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #progress_metric: host=algo-1, completed 67.03296703296704 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319834.4584944, \"EndTime\": 1686319834.6822903, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 60, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4306679.0, \"count\": 1, \"min\": 4306679, \"max\": 4306679}, \"Total Batches Seen\": {\"sum\": 4393.0, \"count\": 1, \"min\": 4393, \"max\": 4393}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=315232.4278187835 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=61, batch=0 train rmse <loss>=0.8904218322466706\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=61, batch=0 train mse <loss>=0.7928510393415179\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=61, batch=0 train absolute_loss <loss>=0.6942276215889085\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:34.897] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 124, \"duration\": 213, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=61, train rmse <loss>=0.9770015300091431\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=61, train mse <loss>=0.9545319896402067\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=61, train absolute_loss <loss>=0.7441487004232972\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319834.6821246, \"EndTime\": 1686319834.8983703, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 215.8071994781494, \"count\": 1, \"min\": 215.8071994781494, \"max\": 215.8071994781494}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #progress_metric: host=algo-1, completed 68.13186813186813 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319834.6825376, \"EndTime\": 1686319834.8985667, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 61, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4377264.0, \"count\": 1, \"min\": 4377264, \"max\": 4377264}, \"Total Batches Seen\": {\"sum\": 4465.0, \"count\": 1, \"min\": 4465, \"max\": 4465}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 63.0, \"count\": 1, \"min\": 63, \"max\": 63}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=326527.1108817022 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=62, batch=0 train rmse <loss>=0.8891704649775998\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=62, batch=0 train mse <loss>=0.7906241157884809\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:34 INFO 140199547356992] #quality_metric: host=algo-1, epoch=62, batch=0 train absolute_loss <loss>=0.6930326465629716\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:35.112] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 126, \"duration\": 210, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=62, train rmse <loss>=0.975776249130363\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=62, train mse <loss>=0.9521392883669203\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=62, train absolute_loss <loss>=0.7429800223323447\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319834.8984303, \"EndTime\": 1686319835.1125479, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 213.69361877441406, \"count\": 1, \"min\": 213.69361877441406, \"max\": 213.69361877441406}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #progress_metric: host=algo-1, completed 69.23076923076923 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319834.8988285, \"EndTime\": 1686319835.1127849, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 62, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4447849.0, \"count\": 1, \"min\": 4447849, \"max\": 4447849}, \"Total Batches Seen\": {\"sum\": 4537.0, \"count\": 1, \"min\": 4537, \"max\": 4537}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=329729.9365276304 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=63, batch=0 train rmse <loss>=0.8879441311299794\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=63, batch=0 train mse <loss>=0.7884447800081741\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=63, batch=0 train absolute_loss <loss>=0.6918587943677691\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:35.328] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 213, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=63, train rmse <loss>=0.9745750161735288\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=63, train mse <loss>=0.9497964621496339\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=63, train absolute_loss <loss>=0.7418348285513865\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319835.1126287, \"EndTime\": 1686319835.3286812, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 215.6505584716797, \"count\": 1, \"min\": 215.6505584716797, \"max\": 215.6505584716797}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #progress_metric: host=algo-1, completed 70.32967032967034 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319835.113007, \"EndTime\": 1686319835.3288872, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 63, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4518434.0, \"count\": 1, \"min\": 4518434, \"max\": 4518434}, \"Total Batches Seen\": {\"sum\": 4609.0, \"count\": 1, \"min\": 4609, \"max\": 4609}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 65.0, \"count\": 1, \"min\": 65, \"max\": 65}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=326792.7468124304 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=64, batch=0 train rmse <loss>=0.886741999740623\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=64, batch=0 train mse <loss>=0.786311374103999\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=64, batch=0 train absolute_loss <loss>=0.690713433432627\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:35.544] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 130, \"duration\": 213, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=64, train rmse <loss>=0.9733971062392325\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=64, train mse <loss>=0.9475019264349116\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=64, train absolute_loss <loss>=0.7407128393530126\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319835.3287451, \"EndTime\": 1686319835.545082, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 215.93213081359863, \"count\": 1, \"min\": 215.93213081359863, \"max\": 215.93213081359863}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #progress_metric: host=algo-1, completed 71.42857142857143 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319835.329123, \"EndTime\": 1686319835.5452654, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 64, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4589019.0, \"count\": 1, \"min\": 4589019, \"max\": 4589019}, \"Total Batches Seen\": {\"sum\": 4681.0, \"count\": 1, \"min\": 4681, \"max\": 4681}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=326421.6252118315 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=65, batch=0 train rmse <loss>=0.8855634066498265\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=65, batch=0 train mse <loss>=0.7842225471972459\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=65, batch=0 train absolute_loss <loss>=0.6895988970934985\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:35.754] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 132, \"duration\": 207, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=65, train rmse <loss>=0.9722418602525644\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=65, train mse <loss>=0.945254234827367\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=65, train absolute_loss <loss>=0.7396128462807007\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319835.5451477, \"EndTime\": 1686319835.7555966, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 210.10684967041016, \"count\": 1, \"min\": 210.10684967041016, \"max\": 210.10684967041016}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #progress_metric: host=algo-1, completed 72.52747252747253 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319835.545454, \"EndTime\": 1686319835.755905, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 65, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4659604.0, \"count\": 1, \"min\": 4659604, \"max\": 4659604}, \"Total Batches Seen\": {\"sum\": 4753.0, \"count\": 1, \"min\": 4753, \"max\": 4753}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 67.0, \"count\": 1, \"min\": 67, \"max\": 67}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=335224.60164365446 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=66, batch=0 train rmse <loss>=0.8844076475277831\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=66, batch=0 train mse <loss>=0.7821768870056275\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=66, batch=0 train absolute_loss <loss>=0.688512483592964\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:35.968] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 209, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=66, train rmse <loss>=0.9711085862820161\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=66, train mse <loss>=0.9430518863506561\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=66, train absolute_loss <loss>=0.7385351768201985\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319835.755673, \"EndTime\": 1686319835.9686642, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 212.42928504943848, \"count\": 1, \"min\": 212.42928504943848, \"max\": 212.42928504943848}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #progress_metric: host=algo-1, completed 73.62637362637362 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319835.75621, \"EndTime\": 1686319835.9688962, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 66, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4730189.0, \"count\": 1, \"min\": 4730189, \"max\": 4730189}, \"Total Batches Seen\": {\"sum\": 4825.0, \"count\": 1, \"min\": 4825, \"max\": 4825}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=331685.2396541255 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=67, batch=0 train rmse <loss>=0.8832742558638852\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=67, batch=0 train mse <loss>=0.7801734110719002\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:35 INFO 140199547356992] #quality_metric: host=algo-1, epoch=67, batch=0 train absolute_loss <loss>=0.6874364472970637\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:36.189] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 136, \"duration\": 218, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=67, train rmse <loss>=0.9699967001759678\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=67, train mse <loss>=0.9408935983522664\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=67, train absolute_loss <loss>=0.7374775940050569\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319835.9687312, \"EndTime\": 1686319836.1896996, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 220.54147720336914, \"count\": 1, \"min\": 220.54147720336914, \"max\": 220.54147720336914}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #progress_metric: host=algo-1, completed 74.72527472527473 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319835.9691339, \"EndTime\": 1686319836.1898575, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 67, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4800774.0, \"count\": 1, \"min\": 4800774, \"max\": 4800774}, \"Total Batches Seen\": {\"sum\": 4897.0, \"count\": 1, \"min\": 4897, \"max\": 4897}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 69.0, \"count\": 1, \"min\": 69, \"max\": 69}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=319655.772861943 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=68, batch=0 train rmse <loss>=0.8821623781931904\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=68, batch=0 train mse <loss>=0.7782104614994655\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=68, batch=0 train absolute_loss <loss>=0.6863711566272636\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:36.409] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 138, \"duration\": 217, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=68, train rmse <loss>=0.9689056167395657\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=68, train mse <loss>=0.938778094149478\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=68, train absolute_loss <loss>=0.7364414580950874\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319836.1897576, \"EndTime\": 1686319836.4102888, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 220.2465534210205, \"count\": 1, \"min\": 220.2465534210205, \"max\": 220.2465534210205}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #progress_metric: host=algo-1, completed 75.82417582417582 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319836.1900203, \"EndTime\": 1686319836.4104514, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 68, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4871359.0, \"count\": 1, \"min\": 4871359, \"max\": 4871359}, \"Total Batches Seen\": {\"sum\": 4969.0, \"count\": 1, \"min\": 4969, \"max\": 4969}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=320086.74020808266 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=69, batch=0 train rmse <loss>=0.8810715732757799\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=69, batch=0 train mse <loss>=0.7762871172346579\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=69, batch=0 train absolute_loss <loss>=0.6853218308876697\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:36.631] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 218, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=69, train rmse <loss>=0.96783474589101\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=69, train mse <loss>=0.9367040953539161\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=69, train absolute_loss <loss>=0.7354285199450828\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319836.4103491, \"EndTime\": 1686319836.6319313, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 221.17376327514648, \"count\": 1, \"min\": 221.17376327514648, \"max\": 221.17376327514648}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #progress_metric: host=algo-1, completed 76.92307692307692 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319836.4107327, \"EndTime\": 1686319836.6321723, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 69, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4941944.0, \"count\": 1, \"min\": 4941944, \"max\": 4941944}, \"Total Batches Seen\": {\"sum\": 5041.0, \"count\": 1, \"min\": 5041, \"max\": 5041}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 71.0, \"count\": 1, \"min\": 71, \"max\": 71}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=318519.83315240603 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=70, batch=0 train rmse <loss>=0.8800012565954101\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=70, batch=0 train mse <loss>=0.7744022116095007\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=70, batch=0 train absolute_loss <loss>=0.684291110432124\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:36.844] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 142, \"duration\": 210, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=70, train rmse <loss>=0.9667834579150496\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=70, train mse <loss>=0.9346702544981805\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=70, train absolute_loss <loss>=0.7344346331078921\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319836.6320078, \"EndTime\": 1686319836.8453207, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 212.81981468200684, \"count\": 1, \"min\": 212.81981468200684, \"max\": 212.81981468200684}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #progress_metric: host=algo-1, completed 78.02197802197803 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319836.632474, \"EndTime\": 1686319836.8455386, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 70, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5012529.0, \"count\": 1, \"min\": 5012529, \"max\": 5012529}, \"Total Batches Seen\": {\"sum\": 5113.0, \"count\": 1, \"min\": 5113, \"max\": 5113}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=331098.0325019767 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=71, batch=0 train rmse <loss>=0.8789506646781313\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=71, batch=0 train mse <loss>=0.7725542709381288\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:36 INFO 140199547356992] #quality_metric: host=algo-1, epoch=71, batch=0 train absolute_loss <loss>=0.6832791794713594\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:37.067] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 144, \"duration\": 220, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=71, train rmse <loss>=0.96575126042872\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=71, train mse <loss>=0.9326754970196613\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=71, train absolute_loss <loss>=0.73345819597353\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319836.8453906, \"EndTime\": 1686319837.06839, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 222.57542610168457, \"count\": 1, \"min\": 222.57542610168457, \"max\": 222.57542610168457}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #progress_metric: host=algo-1, completed 79.12087912087912 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319836.8457863, \"EndTime\": 1686319837.0686343, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 71, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5083114.0, \"count\": 1, \"min\": 5083114, \"max\": 5083114}, \"Total Batches Seen\": {\"sum\": 5185.0, \"count\": 1, \"min\": 5185, \"max\": 5185}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 73.0, \"count\": 1, \"min\": 73, \"max\": 73}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=316567.5771356593 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=72, batch=0 train rmse <loss>=0.8779194137129169\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=72, batch=0 train mse <loss>=0.7707424969740317\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=72, batch=0 train absolute_loss <loss>=0.6822839502837337\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:37.286] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 215, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=72, train rmse <loss>=0.9647375707148226\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=72, train mse <loss>=0.9307185803487373\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=72, train absolute_loss <loss>=0.7325000588109822\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319837.068464, \"EndTime\": 1686319837.2866275, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 217.71526336669922, \"count\": 1, \"min\": 217.71526336669922, \"max\": 217.71526336669922}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #progress_metric: host=algo-1, completed 80.21978021978022 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319837.0688837, \"EndTime\": 1686319837.2868674, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 72, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5153699.0, \"count\": 1, \"min\": 5153699, \"max\": 5153699}, \"Total Batches Seen\": {\"sum\": 5257.0, \"count\": 1, \"min\": 5257, \"max\": 5257}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=323638.3180489981 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=73, batch=0 train rmse <loss>=0.876906976740628\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=73, batch=0 train mse <loss>=0.7689658458563883\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=73, batch=0 train absolute_loss <loss>=0.6813035807619152\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:37.495] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 148, \"duration\": 206, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=73, train rmse <loss>=0.9637419095899247\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=73, train mse <loss>=0.9287984683000345\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=73, train absolute_loss <loss>=0.7315592280762483\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319837.286698, \"EndTime\": 1686319837.495809, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 208.7087631225586, \"count\": 1, \"min\": 208.7087631225586, \"max\": 208.7087631225586}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #progress_metric: host=algo-1, completed 81.31868131868131 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319837.2870755, \"EndTime\": 1686319837.4960036, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 73, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5224284.0, \"count\": 1, \"min\": 5224284, \"max\": 5224284}, \"Total Batches Seen\": {\"sum\": 5329.0, \"count\": 1, \"min\": 5329, \"max\": 5329}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 75.0, \"count\": 1, \"min\": 75, \"max\": 75}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=337623.8745184621 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=74, batch=0 train rmse <loss>=0.8759126478744835\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=74, batch=0 train mse <loss>=0.7672229667064889\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=74, batch=0 train absolute_loss <loss>=0.6803386849416814\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:37.724] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 150, \"duration\": 226, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=74, train rmse <loss>=0.9627638001722482\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=74, train mse <loss>=0.9269141349221086\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=74, train absolute_loss <loss>=0.7306353396119992\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319837.4958718, \"EndTime\": 1686319837.7248147, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 228.53326797485352, \"count\": 1, \"min\": 228.53326797485352, \"max\": 228.53326797485352}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #progress_metric: host=algo-1, completed 82.41758241758242 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319837.49626, \"EndTime\": 1686319837.72495, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 74, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5294869.0, \"count\": 1, \"min\": 5294869, \"max\": 5294869}, \"Total Batches Seen\": {\"sum\": 5401.0, \"count\": 1, \"min\": 5401, \"max\": 5401}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=308539.37663685717 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=75, batch=0 train rmse <loss>=0.8749361379486827\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=75, batch=0 train mse <loss>=0.7655132454885564\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=75, batch=0 train absolute_loss <loss>=0.6793938066877829\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:37.958] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 231, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=75, train rmse <loss>=0.9618026398540952\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=75, train mse <loss>=0.9250643180303064\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=75, train absolute_loss <loss>=0.7297279891780152\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319837.724862, \"EndTime\": 1686319837.9590704, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 233.76822471618652, \"count\": 1, \"min\": 233.76822471618652, \"max\": 233.76822471618652}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #progress_metric: host=algo-1, completed 83.51648351648352 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319837.725277, \"EndTime\": 1686319837.9593744, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 75, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5365454.0, \"count\": 1, \"min\": 5365454, \"max\": 5365454}, \"Total Batches Seen\": {\"sum\": 5473.0, \"count\": 1, \"min\": 5473, \"max\": 5473}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 77.0, \"count\": 1, \"min\": 77, \"max\": 77}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=301403.9639889112 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=76, batch=0 train rmse <loss>=0.8739768392438176\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=76, batch=0 train mse <loss>=0.7638355155346139\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:37 INFO 140199547356992] #quality_metric: host=algo-1, epoch=76, batch=0 train absolute_loss <loss>=0.6784598582707181\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:38.187] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 154, \"duration\": 225, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=76, train rmse <loss>=0.960858118702182\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=76, train mse <loss>=0.9232483242758963\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=76, train absolute_loss <loss>=0.7288372791135922\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319837.9591565, \"EndTime\": 1686319838.1884527, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 228.55806350708008, \"count\": 1, \"min\": 228.55806350708008, \"max\": 228.55806350708008}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #progress_metric: host=algo-1, completed 84.61538461538461 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319837.9598691, \"EndTime\": 1686319838.1886964, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 76, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5436039.0, \"count\": 1, \"min\": 5436039, \"max\": 5436039}, \"Total Batches Seen\": {\"sum\": 5545.0, \"count\": 1, \"min\": 5545, \"max\": 5545}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=308313.81337354553 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=77, batch=0 train rmse <loss>=0.8730344216612005\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=77, batch=0 train mse <loss>=0.7621891014053068\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=77, batch=0 train absolute_loss <loss>=0.6775346291616888\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:38.399] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 156, \"duration\": 208, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=77, train rmse <loss>=0.9599297008037112\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=77, train mse <loss>=0.9214650304851024\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=77, train absolute_loss <loss>=0.7279626670204604\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319838.1885238, \"EndTime\": 1686319838.40036, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 211.198091506958, \"count\": 1, \"min\": 211.198091506958, \"max\": 211.198091506958}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #progress_metric: host=algo-1, completed 85.71428571428571 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319838.1891363, \"EndTime\": 1686319838.4006178, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 77, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5506624.0, \"count\": 1, \"min\": 5506624, \"max\": 5506624}, \"Total Batches Seen\": {\"sum\": 5617.0, \"count\": 1, \"min\": 5617, \"max\": 5617}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 79.0, \"count\": 1, \"min\": 79, \"max\": 79}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=333567.25410177774 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=78, batch=0 train rmse <loss>=0.8721082358452308\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=78, batch=0 train mse <loss>=0.7605727750290807\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=78, batch=0 train absolute_loss <loss>=0.6766277597223969\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:38.620] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 217, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=78, train rmse <loss>=0.9590169082563208\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=78, train mse <loss>=0.9197134303215124\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=78, train absolute_loss <loss>=0.7271050986842823\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319838.4004338, \"EndTime\": 1686319838.6210604, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 219.89989280700684, \"count\": 1, \"min\": 219.89989280700684, \"max\": 219.89989280700684}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #progress_metric: host=algo-1, completed 86.81318681318682 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319838.401138, \"EndTime\": 1686319838.6212828, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 78, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5577209.0, \"count\": 1, \"min\": 5577209, \"max\": 5577209}, \"Total Batches Seen\": {\"sum\": 5689.0, \"count\": 1, \"min\": 5689, \"max\": 5689}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=320472.726272912 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=79, batch=0 train rmse <loss>=0.8711979811559245\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=79, batch=0 train mse <loss>=0.7589859223701585\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=79, batch=0 train absolute_loss <loss>=0.6757339078415807\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:38.833] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 160, \"duration\": 209, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=79, train rmse <loss>=0.9581194319015186\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=79, train mse <loss>=0.9179928457872888\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=79, train absolute_loss <loss>=0.726263831292333\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319838.6211197, \"EndTime\": 1686319838.833975, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 212.22186088562012, \"count\": 1, \"min\": 212.22186088562012, \"max\": 212.22186088562012}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #progress_metric: host=algo-1, completed 87.91208791208791 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319838.6217268, \"EndTime\": 1686319838.834241, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 79, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5647794.0, \"count\": 1, \"min\": 5647794, \"max\": 5647794}, \"Total Batches Seen\": {\"sum\": 5761.0, \"count\": 1, \"min\": 5761, \"max\": 5761}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 81.0, \"count\": 1, \"min\": 81, \"max\": 81}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=331948.5438909258 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=80, batch=0 train rmse <loss>=0.8703030725926119\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=80, batch=0 train mse <loss>=0.757427438164141\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:38 INFO 140199547356992] #quality_metric: host=algo-1, epoch=80, batch=0 train absolute_loss <loss>=0.6748586612448126\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:39.055] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 162, \"duration\": 218, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=80, train rmse <loss>=0.9572367936275192\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=80, train mse <loss>=0.9163022790742938\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=80, train absolute_loss <loss>=0.7254384375780508\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319838.8340497, \"EndTime\": 1686319839.0561137, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 221.34065628051758, \"count\": 1, \"min\": 221.34065628051758, \"max\": 221.34065628051758}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #progress_metric: host=algo-1, completed 89.01098901098901 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319838.8347452, \"EndTime\": 1686319839.0563421, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 80, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5718379.0, \"count\": 1, \"min\": 5718379, \"max\": 5718379}, \"Total Batches Seen\": {\"sum\": 5833.0, \"count\": 1, \"min\": 5833, \"max\": 5833}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=318358.85055073386 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=81, batch=0 train rmse <loss>=0.8694232397278263\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=81, batch=0 train mse <loss>=0.7558967697788292\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=81, batch=0 train absolute_loss <loss>=0.673991519920303\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:39.321] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 263, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=81, train rmse <loss>=0.956368594445581\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=81, train mse <loss>=0.9146408884418162\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=81, train absolute_loss <loss>=0.7246270182946638\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319839.0561862, \"EndTime\": 1686319839.3223035, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 265.49768447875977, \"count\": 1, \"min\": 265.49768447875977, \"max\": 265.49768447875977}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #progress_metric: host=algo-1, completed 90.10989010989012 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319839.0567806, \"EndTime\": 1686319839.3225, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 81, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5788964.0, \"count\": 1, \"min\": 5788964, \"max\": 5788964}, \"Total Batches Seen\": {\"sum\": 5905.0, \"count\": 1, \"min\": 5905, \"max\": 5905}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 83.0, \"count\": 1, \"min\": 83, \"max\": 83}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=265532.75432823267 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=82, batch=0 train rmse <loss>=0.8685580335038844\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=82, batch=0 train mse <loss>=0.7543930575641348\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=82, batch=0 train absolute_loss <loss>=0.6731355540469379\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:39.540] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 166, \"duration\": 215, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=82, train rmse <loss>=0.9555144180769911\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=82, train mse <loss>=0.913007803153011\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=82, train absolute_loss <loss>=0.7238300390859698\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319839.3223705, \"EndTime\": 1686319839.540712, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 217.75126457214355, \"count\": 1, \"min\": 217.75126457214355, \"max\": 217.75126457214355}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #progress_metric: host=algo-1, completed 91.20879120879121 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319839.3229394, \"EndTime\": 1686319839.5408766, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 82, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5859549.0, \"count\": 1, \"min\": 5859549, \"max\": 5859549}, \"Total Batches Seen\": {\"sum\": 5977.0, \"count\": 1, \"min\": 5977, \"max\": 5977}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=323745.55243048415 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=83, batch=0 train rmse <loss>=0.8677071792303314\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=83, batch=0 train mse <loss>=0.7529157488878584\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=83, batch=0 train absolute_loss <loss>=0.6722992373184419\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:39.756] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 168, \"duration\": 213, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=83, train rmse <loss>=0.954674018533256\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=83, train mse <loss>=0.9114024816624355\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=83, train absolute_loss <loss>=0.7230482436175211\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319839.5407665, \"EndTime\": 1686319839.756577, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 215.3158187866211, \"count\": 1, \"min\": 215.3158187866211, \"max\": 215.3158187866211}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #progress_metric: host=algo-1, completed 92.3076923076923 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319839.5412347, \"EndTime\": 1686319839.7568445, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 83, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5930134.0, \"count\": 1, \"min\": 5930134, \"max\": 5930134}, \"Total Batches Seen\": {\"sum\": 6049.0, \"count\": 1, \"min\": 6049, \"max\": 6049}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 85.0, \"count\": 1, \"min\": 85, \"max\": 85}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=327192.19287494407 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=84, batch=0 train rmse <loss>=0.8668700816648369\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=84, batch=0 train mse <loss>=0.7514637384856011\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=84, batch=0 train absolute_loss <loss>=0.6714772890271316\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:39.984] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 224, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=84, train rmse <loss>=0.9538469243846657\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=84, train mse <loss>=0.9098239551580861\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=84, train absolute_loss <loss>=0.7222800055473356\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319839.7566652, \"EndTime\": 1686319839.9847417, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 227.24056243896484, \"count\": 1, \"min\": 227.24056243896484, \"max\": 227.24056243896484}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #progress_metric: host=algo-1, completed 93.4065934065934 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319839.7574744, \"EndTime\": 1686319839.9849894, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 84, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6000719.0, \"count\": 1, \"min\": 6000719, \"max\": 6000719}, \"Total Batches Seen\": {\"sum\": 6121.0, \"count\": 1, \"min\": 6121, \"max\": 6121}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=310087.3297742753 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=85, batch=0 train rmse <loss>=0.8660464970939732\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=85, batch=0 train mse <loss>=0.7500365351287412\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:39 INFO 140199547356992] #quality_metric: host=algo-1, epoch=85, batch=0 train absolute_loss <loss>=0.6706688495229187\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-06-09 14:10:43 Uploading - Uploading generated training model\u001b[34m[2023-06-09 14:10:40.198] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 172, \"duration\": 211, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=85, train rmse <loss>=0.9530328320170279\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=85, train mse <loss>=0.9082715789023966\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=85, train absolute_loss <loss>=0.7215251679551593\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319839.9848177, \"EndTime\": 1686319840.1993034, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 213.8357162475586, \"count\": 1, \"min\": 213.8357162475586, \"max\": 213.8357162475586}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #progress_metric: host=algo-1, completed 94.50549450549451 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319839.9854412, \"EndTime\": 1686319840.199531, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 85, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6071304.0, \"count\": 1, \"min\": 6071304, \"max\": 6071304}, \"Total Batches Seen\": {\"sum\": 6193.0, \"count\": 1, \"min\": 6193, \"max\": 6193}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 87.0, \"count\": 1, \"min\": 87, \"max\": 87}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=329531.74865401245 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=86, batch=0 train rmse <loss>=0.865236109268159\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=86, batch=0 train mse <loss>=0.7486335247815015\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=86, batch=0 train absolute_loss <loss>=0.6698693135374748\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:40.421] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 174, \"duration\": 219, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=86, train rmse <loss>=0.9522314366847108\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=86, train mse <loss>=0.9067447090106284\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=86, train absolute_loss <loss>=0.720782943681239\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319840.1993737, \"EndTime\": 1686319840.4222605, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 222.23901748657227, \"count\": 1, \"min\": 222.23901748657227, \"max\": 222.23901748657227}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #progress_metric: host=algo-1, completed 95.6043956043956 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319840.1999946, \"EndTime\": 1686319840.4224985, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 86, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6141889.0, \"count\": 1, \"min\": 6141889, \"max\": 6141889}, \"Total Batches Seen\": {\"sum\": 6265.0, \"count\": 1, \"min\": 6265, \"max\": 6265}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=317065.618088235 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=87, batch=0 train rmse <loss>=0.8644386001377454\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=87, batch=0 train mse <loss>=0.7472540934081049\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=87, batch=0 train absolute_loss <loss>=0.6690786810707998\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:40.639] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 214, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=87, train rmse <loss>=0.9514423662168956\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=87, train mse <loss>=0.9052425762324052\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=87, train absolute_loss <loss>=0.7200529071646677\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319840.4223342, \"EndTime\": 1686319840.6401694, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 217.13733673095703, \"count\": 1, \"min\": 217.13733673095703, \"max\": 217.13733673095703}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #progress_metric: host=algo-1, completed 96.7032967032967 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319840.423008, \"EndTime\": 1686319840.6403358, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 87, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6212474.0, \"count\": 1, \"min\": 6212474, \"max\": 6212474}, \"Total Batches Seen\": {\"sum\": 6337.0, \"count\": 1, \"min\": 6337, \"max\": 6337}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 89.0, \"count\": 1, \"min\": 89, \"max\": 89}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=324650.4874226768 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=88, batch=0 train rmse <loss>=0.8636534721489202\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=88, batch=0 train mse <loss>=0.7458973199548856\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=88, batch=0 train absolute_loss <loss>=0.6683103995064135\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:40.859] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 178, \"duration\": 216, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=88, train rmse <loss>=0.9506653494341955\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=88, train mse <loss>=0.9037646066148409\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=88, train absolute_loss <loss>=0.7193353398385103\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319840.6402311, \"EndTime\": 1686319840.8596137, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 218.8258171081543, \"count\": 1, \"min\": 218.8258171081543, \"max\": 218.8258171081543}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #progress_metric: host=algo-1, completed 97.8021978021978 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319840.640763, \"EndTime\": 1686319840.8597891, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 88, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6283059.0, \"count\": 1, \"min\": 6283059, \"max\": 6283059}, \"Total Batches Seen\": {\"sum\": 6409.0, \"count\": 1, \"min\": 6409, \"max\": 6409}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=322132.1931389873 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=89, batch=0 train rmse <loss>=0.8628805811953094\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=89, batch=0 train mse <loss>=0.744562897403955\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:40 INFO 140199547356992] #quality_metric: host=algo-1, epoch=89, batch=0 train absolute_loss <loss>=0.6675507758464851\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:41.075] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 180, \"duration\": 213, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, epoch=89, train rmse <loss>=0.949900057691132\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, epoch=89, train mse <loss>=0.9023101196016158\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, epoch=89, train absolute_loss <loss>=0.7186308395348218\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319840.8596876, \"EndTime\": 1686319841.0763958, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 216.14336967468262, \"count\": 1, \"min\": 216.14336967468262, \"max\": 216.14336967468262}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #progress_metric: host=algo-1, completed 98.9010989010989 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319840.8602314, \"EndTime\": 1686319841.0766156, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 89, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6353644.0, \"count\": 1, \"min\": 6353644, \"max\": 6353644}, \"Total Batches Seen\": {\"sum\": 6481.0, \"count\": 1, \"min\": 6481, \"max\": 6481}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=326009.6968121949 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, epoch=90, batch=0 train rmse <loss>=0.862119604068679\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, epoch=90, batch=0 train mse <loss>=0.743250211719536\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, epoch=90, batch=0 train absolute_loss <loss>=0.6668028802699006\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:41.292] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 213, \"num_examples\": 72, \"num_bytes\": 4517440}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, epoch=90, train rmse <loss>=0.9491462574304951\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, epoch=90, train mse <loss>=0.9008786179943157\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, epoch=90, train absolute_loss <loss>=0.7179379555940788\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, train rmse <loss>=0.9491462574304951\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, train mse <loss>=0.9008786179943157\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, train absolute_loss <loss>=0.7179379555940788\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319841.0764525, \"EndTime\": 1686319841.2933698, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 216.4590358734131, \"count\": 1, \"min\": 216.4590358734131, \"max\": 216.4590358734131}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319841.076882, \"EndTime\": 1686319841.2936175, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 90, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6424229.0, \"count\": 1, \"min\": 6424229, \"max\": 6424229}, \"Total Batches Seen\": {\"sum\": 6553.0, \"count\": 1, \"min\": 6553, \"max\": 6553}, \"Max Records Seen Between Resets\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Max Batches Seen Between Resets\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 70585.0, \"count\": 1, \"min\": 70585, \"max\": 70585}, \"Number of Batches Since Last Reset\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #throughput_metric: host=algo-1, train throughput=325474.2388119757 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 WARNING 140199547356992] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319841.2934487, \"EndTime\": 1686319841.2971947, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 3.252744674682617, \"count\": 1, \"min\": 3.252744674682617, \"max\": 3.252744674682617}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] Saved checkpoint to \"/tmp/tmp_hoxqmqe/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:41.302] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 20911, \"num_examples\": 1, \"num_bytes\": 63616}\u001b[0m\n",
      "\u001b[34m[2023-06-09 14:10:41.369] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 67, \"num_examples\": 31, \"num_bytes\": 1936064}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319841.302142, \"EndTime\": 1686319841.369969, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 30251.0, \"count\": 1, \"min\": 30251, \"max\": 30251}, \"Total Batches Seen\": {\"sum\": 31.0, \"count\": 1, \"min\": 31, \"max\": 31}, \"Max Records Seen Between Resets\": {\"sum\": 30251.0, \"count\": 1, \"min\": 30251, \"max\": 30251}, \"Max Batches Seen Between Resets\": {\"sum\": 31.0, \"count\": 1, \"min\": 31, \"max\": 31}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 30251.0, \"count\": 1, \"min\": 30251, \"max\": 30251}, \"Number of Batches Since Last Reset\": {\"sum\": 31.0, \"count\": 1, \"min\": 31, \"max\": 31}}}\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #test_score (algo-1) : ('rmse', 0.9069811409492536)\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #test_score (algo-1) : ('mse', 0.8226147900376098)\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #test_score (algo-1) : ('absolute_loss', 0.7209633612237896)\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, test rmse <loss>=0.9069811409492536\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, test mse <loss>=0.8226147900376098\u001b[0m\n",
      "\u001b[34m[06/09/2023 14:10:41 INFO 140199547356992] #quality_metric: host=algo-1, test absolute_loss <loss>=0.7209633612237896\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1686319841.2972486, \"EndTime\": 1686319841.3709018, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 17.616748809814453, \"count\": 1, \"min\": 17.616748809814453, \"max\": 17.616748809814453}, \"totaltime\": {\"sum\": 21004.16588783264, \"count\": 1, \"min\": 21004.16588783264, \"max\": 21004.16588783264}}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-06-09 14:10:54 Completed - Training job completed\n",
      "Training seconds: 213\n",
      "Billable seconds: 75\n",
      "Managed Spot Training savings: 64.8%\n"
     ]
    }
   ],
   "source": [
    "# New Hyperparameters\n",
    "# Reference: Supported channels by algorithm\n",
    "#   https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n",
    "estimator.fit({'train':s3_training_file_location, 'test': s3_test_file_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: http://sagemaker.readthedocs.io/en/latest/estimators.html\n",
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                             instance_type='ml.m5.xlarge',\n",
    "                             endpoint_name = job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run Predictions\n",
    "### Dense and Sparse Formats\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html\n",
    "\n",
    "\n",
    "Define a custom serializer for the data.\n",
    "\n",
    "This step defines a function fm_sparse_serializer(data), which converts data into a specific JSON format expected by the Factorization Machines model in SageMaker. This function iterates over a list of arrays, where each array contains feature values. For each array, it creates a dictionary in the format {'keys': column_list, 'shape': [dim_movie], 'values': value_list} and appends this dictionary to a list of instances. The final result is a JSON string which represents a list of instances, where each instance contains a sparse representation of its features. This custom serializer is necessary because the Factorization Machines model expects input in this specific format.\n",
    "\n",
    "Set the custom serializer and a JSONDeserializer for the model.\n",
    "\n",
    "After the fm_sparse_serializer function is defined, it's set as the serializer for the predictor object, which was created by the estimator.deploy() call. By doing this, you're specifying that whenever you pass data to the predictor.predict() method, the data should be serialized (i.e., converted into a format that can be transmitted over the network) using this function.\n",
    "\n",
    "The content type for the serializer is then set to 'application/json', which indicates to the SageMaker endpoint that the data being sent is in JSON format.\n",
    "\n",
    "Finally, the deserializer for the predictor is set to JSONDeserializer(). This means that when the predictor receives responses from the SageMaker endpoint, it should interpret them as JSON and convert them into Python objects.\n",
    "\n",
    "Test the deployed model with some data.\n",
    "\n",
    "In this step, the code reads from the testing data file, parses out user ID and movie ID pairs, and uses the deployed model to predict ratings for those pairs. The fm_sparse_serializer is used to convert the feature arrays into the required JSON format, and the predictor.predict() method is used to make the predictions. The results are then printed out, comparing the actual rating with the predicted rating from the model.\n",
    "\n",
    "Here's a closer look at this process in the provided script:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "with open(r'ml-latest-small/user_movie_test.svm','r') as f:\n",
    "    for i in range(3):\n",
    "        rating = f.readline().split()\n",
    "        print(f\"Movie {rating}\")\n",
    "        userID = rating[1].split(':')[0]\n",
    "        movieID = rating[2].split(':')[0]\n",
    "        predicted_rating = predictor.predict([np.array([int(userID),int(movieID)])])\n",
    "        print(f'  Actual Rating:\\t{rating[0]}')\n",
    "        print(f\"  Predicted Rating:\\t{predicted_rating['predictions'][0]['score']}\")\n",
    "        print()\n",
    "This part of the code opens the test file, reads the first three lines (which each represent a movie rating), and splits each line into individual components. It then uses the predictor to make a prediction for the user and movie specified in each line. After making the prediction, it prints out the actual rating from the test data and the predicted rating from the model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def fm_sparse_serializer(data):\n",
    "    js = {'instances': []}\n",
    "    for row in data:\n",
    "        \n",
    "        column_list = row.tolist()\n",
    "        value_list = np.ones(len(column_list),dtype=int).tolist()\n",
    "       \n",
    "        js['instances'].append({'data':{'features': { 'keys': column_list, 'shape':[dim_movie], 'values': value_list}}})\n",
    "    return json.dumps(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDK 2\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/factorization_machines_mnist/factorization_machines_mnist.ipynb\n",
    "\n",
    "# Specify custom serializer\n",
    "predictor.serializer.serialize = fm_sparse_serializer\n",
    "predictor.serializer.content_type = 'application/json'\n",
    "\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_sparse_serializer([np.array([341,1416])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test with few entries from test file\n",
    "# Movie dataset is updated regularly...so, instead of hard coding userid and movie id, let's\n",
    "# use actual values\n",
    "\n",
    "# Each row is in this format: ['2.5', '426:1', '943:1']\n",
    "# ActualRating, UserID, MovieID\n",
    "\n",
    "with open(r'ml-latest-small/user_movie_test.svm','r') as f:\n",
    "    for i in range(3):\n",
    "        rating = f.readline().split()\n",
    "        print(f\"Movie {rating}\")\n",
    "        userID = rating[1].split(':')[0]\n",
    "        movieID = rating[2].split(':')[0]\n",
    "        predicted_rating = predictor.predict([np.array([int(userID),int(movieID)])])\n",
    "        print(f'  Actual Rating:\\t{rating[0]}')\n",
    "        print(f\"  Predicted Rating:\\t{predicted_rating['predictions'][0]['score']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ensure Training, Test and Validation data are in S3 Bucket\n",
    "2. Select Algorithm Container Registry Path - Path varies by region\n",
    "3. Configure Estimator for training - Specify Algorithm container, instance count, instance type, model output location\n",
    "4. Specify algorithm specific hyper parameters\n",
    "5. Train model\n",
    "6. Deploy model - Specify instance count, instance type and endpoint name\n",
    "7. Run Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
